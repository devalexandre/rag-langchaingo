You. Hi. So today I want to show you one of the hottest skills in 2024, which is how to build retrieval augmented generation systems using Python using the OpenAI API. Specifically, we're going to be using the GPT 3.5 turbo model and how to use land chain. And the reason I picked Lanchain is because is by far the most popular framework to build LLM applications. Now, by the end of this video, I not only want you to understand how to put together the code, but I want to explain. I want you to leave understanding the reasoning behind every single component that we use to build this solution. So let me go here, let me show you really quick what you are going to find. I'm going to be using visual studio code throughout this video to run the code. The code is in a Jupyter notebook. Now I'm using the extension, the Jupyter extension within visual studio code to run my code within visual studio. But if you want to run it outside, you can just install Jupyter and do it outside. You will find a link in the description of this video for the repo containing all of the code. This is the repository here and there is a quick setup instruction. It's very simple to follow. So basically the first step will be installing Python environment, a virtual environment, and then running the requirements. There are a bunch of libraries that we're going to need to run, so all of them are listed within the requirements DXT file. The second step is just to create a pine cone account. I'm going to explain what Pine cone is when we get to that point and then copy the API key. And finally is to create a m file. So it's an environment file where you are going to host or save the OpenAI API key, the Pine cone API key and the Pine cone API environment. You are going to get those values from within Pine cone. Now this, obviously we're going to be using the GPT 3.5 turbo, which is very cheap, but you're going to need an API account with OpenAI and you're going to need access to GPT in order to do this. You're probably going to have to have a credit card there to do this. I promise, just running this code is just going to cost you cents if anything. It's just very light, but just be aware of that. So after you do the installation, you can open this rack file, and this is sort of like the notebook that is going to show you step by step work we are going to be building here. So here, this diagram here represents the application that I want to solve together with you. Okay? It's a very simple application. We're going to start with a YouTube video, any YouTube video. And what I want to do is being able to, using a model, an LLM model or the GPT 3.5 model, I want to be able to ask questions about that video. Now you can imagine a student that has the videos of a class that it's going through, and the student wants to just ask questions that are going to be answered from the content of the video. That's the application that we're going to build together here. So at the end of today, you're going to be able to use your own video, whatever video you want. That's what you're going to be able to use. For this example, I'm using this particular interview with Andrew Carpati and Lex Freeman. It's a 3 hours and a half interview, so it's a long, long video. And what we're going to be doing is asking questions from this interview. Okay? So that's sort of like the process. So going back to this diagram, you'll see that the idea of how to do this, it's kind of simple, right? We're going to grab the YouTube video, we're going to generate a transcription out of that YouTube video, going to get all of the text, the entire transcription of the video, and together with a specific question, we can give those two pieces of information to a model in order to get an answer. Okay, so the idea will be something like, hey, I don't know, what is artificial intelligence? That will be the question and then answer the question from this transcript. So we're going to give the entire context of the video or content of the video to the model, and the model will look up what artificial intelligence is within that transcript and will give us an answer. So, very simple. So how do we accomplish this? How do we do this? Well, I'm going to start this notebook here by just loading, using the M library here, I'm going to be loading the environment variables in memory. Remember, we added the OpenAI API key to those variables, so I need to load them. That's what I'm loading here because I'm going to be setting up a model and I'm going to need that API key. And this is the URL of the video. Again, this is the interview between Lex Freeman and Andrew Corpati. That's the video that I have here. You can use your own video. Okay, so I'm going to load this all good. And now I'm going to set up the model. Okay, so this is the main component that we're going to be using. If I go back to the diagram that I have here, that's this blue square here, that's going to be our model. Like I said before, we're going to be using the GPT 3.5 turbo model. You can use GPT four if you wanted to. GPT 3.5 is good enough here. Notice I'm using Lanchain here for the first time. Lanchain supports multiple models. Like you can use Lanchain with any sort of model. You can use it with llama open source models, you can use it with Mixtrawl, you can use it with Gemini, you can use it with any sort of model. Specifically here, I'm using the chat OpenAI class. It's a pretty fine, it's an out of the box class that's going to give me access to OpenAI's models. And I'm initializing that class with the key and with the name of the model. Okay, so after running this now we have a variable here, the variable model that will give us access, direct access to that model. So let's try it out. Let's test it. So I can just invoke that model with just a question. And this is just to see whether the model is actually working or not. You can put here any question. I specifically ran it using a question. What MLB team won the World Series during the Covid-19 pandemic? And you can see here the answer coming back from the model. It's an AI message with a content inside that says the Los Angeles Dodgers won the World Series, blah, blah, blah, blah, blah. Okay. That answer is correct. So this model is clearly working. You're going to notice something here that's going to be the same for many different components from lang chain, and it's that invoke function. So all of these components, we can call them using an invoke function. And we're going to pass a query inside and those components are going to do what they're supposed to do. All right, so so far we have a model and we can ask anything we want to that model. Let me add just a new line here. I'm going to add something invoke. How much is two plus two? Just to make sure this is actually working. And you get here back. Okay, so the content is two plus two equals four. Okay. So this model is clearly working. That's fine. The only thing that I don't like at this point is that as you can see, the output coming from the model is an AI message. So it's an object. What's coming back? I would like just to get a string back. Unfortunately, Langchain supports the concept of a parser and this is the way it works. It's just very simple. We have our query, which is how much is two plus two. We send that to the model, the model is going to give us back a response and we can take that response through a parser, which is going to be a class that's going to decide how to format, how to output that result. Okay, in my case here, for what we care about, I'm going to be using a string output parser, which is the name suggest is very simple. It's a very simple class that is going to take that AI message instance and turn it into just a string. And here for the first time, we start seeing the main idea of LAn chain, which is precisely the chaining portion of their name. So we can start chaining components together in order to accomplish something or a task that's a little bit more complex than what a single component can do by itself. So notice here I'm creating the parser and it's just a string output parser. Again, this is just going to take whatever the input is and turn it into a flat string. And then I'm going to create a chain. And a chain is putting together the model connected to the parser. And this is the symbol in python. The pipe symbol is how we're going to make that connection. So I'm saying, okay, just call the model and take the output of that model and that's going to be the input of the next component. In this case, it's going to be the parser. And let's ask the question again. So now we're going to invoke it with the same question we asked before, what MLB team won the World Series. And when we run this now, we are going to get the same answer back. But now notice it's just a string. It's not an AI message anymore, it's just a string. And this is going to make things a little bit clearer. Okay. All right, so let's take this to another level. If we go back up and we see the diagram, what we want to send to the model are two pieces of information. We want to send a question, but we also want to send a transcript. So we want to ask the model, answer the question from this transcript or this context. Right. So we need to combine those two pieces of information in the prompt that we're going to use with the model. Okay, so in order to do that, I could write that just in a string, but we're going to introduce a new component from Langchain in order to set up a prompt. It's going to be a little bit more complex than just a string. So here is our prompt. And because the GPT 3.5 turbo model is a chat model, it's not a completion model, but a chat model. I'm going to be using the chat prompt template class from land chain. Again, land chain supports a bunch of prompts, by the way. Here I have the Lanchain documentation with all of the models here, the modules here, and you can go through the modules and you can find everything that they support. I'm just going to focus on the ones that I'm using here. So the chat prompt template here is the string that I'm going to be using with the model. And it's very simple. It says answer the question based on the context below. If you cannot answer the question, reply I don't know. So it's that simple. And then I give the model the context, and we're going to be using this context to inject the transcript from the video, and I'm going to be giving the model a question. Okay, so, very simple. This is my template string, and now I can create my prompt object, which is a chat prompt template from this string here. Notice the syntax that I'm using to specify context and question. That syntax is telling Lanchain to create two input variables. And you're going to see how we pass those right here. Notice that now I can say prompt, which is the variable that we just created, format. And then I pass the context. And then I pass a question. Okay, so these two correspond to these two variables that now I can pass these two arguments or parameters that now I can use together with the format function in order to generate the full template. Okay, now I'm using here a sample context. I'm saying Mary's sister is Susanna. And the question would be, who is Mary'sister? And again, this is just a sample. And when we execute this, let me do that again. You're going to see the output from that prompt or not the output. Remember here we're not talking to the model, we're just generating formatting that prompt. It says, well, the human is saying, answer the question based on the context below, blah, blah, blah. And then over here, you're going to see the context says Mary's sister is Susanna. And then the question is going to be, who is Mary's sister? So it's sort of like putting together or replacing those variables, context and question by the values that we are passing here. So, very straightforward. This is good. So if we chain now together, if we take that prompt and chain it together with the model and with the parser, we're going to get something that looks like this. We're going to get something that looks like this. I apologize about that. I hit the microphone for some reason. So we're going to get the prompt right, expecting two parameters, the question and the context that prompt, the output of that prompt, which is basically formatting the question, the context. With the overall template, that output is going to go to the model. That's going to be the query that we're going to send the model. The model is going to process that query. It's going to take the response, take it to a parser. And finally, that parser is going to parse out that answer, that what we want. So this is sort of like the full chain so far. So let's see that chain in action. You're going to see that I'm going to be using the pipe symbol like we did before. I'm going to be chaining the prompt with the model and then with the parser. And again, the output of the prompt becomes the input of the model. The output of the model becomes the input of the parser. And when I invoke the chain now, we need to pass two parameters, because the prompt is expecting those two parameters. So I need to supply those two parameters to the prompt. So I'm going to invoke the chain with the parameter context. When I say, hey, Mary'sister is Susana. That's the example context that I'm showing the model. And a question. And that question is, who is Mary's sister? And now when we run this, you'll see that the model is returning the right answer. So the model is basically what we sent to the model is, hey, answer this question from the context below. And the context is Mary'sister is Susanna. And the question was, who is Mary'sister? The model is just going to process that, and it's going to say, susana. Pretty awesome. This is how little it takes, right? So, obviously, after defining the prompt, the model and the parser, my chain is just putting together every single component. Okay, so this is good. Before we get into the transcript of the video, let me show you how powerful chains can get. Because you can now combine different chains together, you can create very complex systems by combining different chains together. So I have this section here. Again, this is sort of like a small rabbit hole. Doesn't necessarily is going to help us solve the YouTube problem, but I wanted to show the power of the chains here. I'm creating here a new prompt template that I'm calling translation prompt. We already have a prompt which is the prompt of answer from the context below. This is a new template that says translate one parameter answer to another parameter, which is language. So this is a new prompt that I just created here. And now I'm going to combine this prompt with a model to perform what the prompt is doing with the previous chain. Okay, so this is what that is going to look like. Looks like a little bit complex, but I promise it's very simple. So we have our original chain at the top question. Context goes into a prompt, the prompt goes into a model, the model goes into a parser. But now I'm going to use the output from the parser, which is that answer that's coming from the parser. And that is going to be one of the inputs of my second chain that I'm going to create now. So you will see the answer going into the translation prompt, just the one that I just showed you. And I'm going to need to specify a language as well. So I specify a language, I'm going to use the answer coming from the previous chain, and then I'm going to take that through a model. I'm going to get the answer from the model and I'm going to parse that answer out to get my final response. So if you haven't noticed already, what I'm doing here is I'm creating a translation chain, or I'm going to be creating a translation chain that is going to translate the answer from the first chain into any language I want. Okay, so this is what that translation chain is going to look like. I have an answer and I have a language. These are the parameters that I'm going to be expecting from outside. Okay? And this is sort of like a little bit of syntax sugar to specify parameters here as part of this chain. Notice how I'm using the curly braces. The answer parameter is coming from chain, and Shane is the previous chain that we defined before. Okay, that is where the answer is coming from. And then I have language which is coming from, and I'm just using the item getter function to grab the language from the invocation of the shane. So whenever you invoke a Shane and you pass parameters, if I use itemgetter here, I can just grab that parameter from the invocation of the Shane. So these are my two parameters. And then I'm going to get all of these two things. I'm going to get them into the translation prompt, which is the prompt that says, translate whatever we get here into this language here. Okay, that's my translation prompt. We defined it before. And then I'm going to take that into the model, which is the GPT 3.5. And then I'm going to take that through the string output parser. Okay. And now I'm going to invoke the chain. So the context. Why do I need a context here? Well, remember, we are chaining a chain, so we still need to pass the context here. So that context gets used by the original chain. The context says, mary's sister is Susanna. She doesn't have any more siblings. Okay, the question is, how many sisters does Mary have? And the language is Spanish. Okay, notice I have three inputs. Let's check the diagram here really quick. You're going to see the question, the context, and the language. Those are the three inputs that my overall chain is expecting. Like the big one here, right? Two of them will go to the previous chain. The third one will go to the translation chain. Why? I don't need an input for answer because answer is coming from the output of the first chain. All right, so let me run this. Just make sure it still works. It doesn't. It says that translation prompt is not defined. That is because I forgot to run this line here. Do that. Now rerun this. And here you go. Maria tiene unarmana Susana. Very nice way of answering this question, but it's correct. So just here, hopefully, this makes clear how you can have separate chains doing different things and you can connect them together in order to accomplish something that is much more complex, which is, I think it's really cool. All right, let's get back to the YouTube video problem that we have. And so far, what we have right now is we have a template that expects two variables, the question and the context. We have a model. We have that template connected to a model, and that model connected to a parser, to an output parser. So I think those are the big components that we need. What we need to do right now is just grab the YouTube video, transcribe that YouTube video, get the text, and use the text as the context. And that's it. That's pretty much it, right? Well, not really, but we're going to get there. So to transcribe the video, I'm not going to spend too much time on how to transcribe the video. I'm going to explain this really quick, but basically I'm using the Whisper library. And whisper is you can get a link here. So let me just open this link. I don't even know how to open a link here. Yeah, let me grab this link. I don't want to click on the link because it's going to open chrome and I'm going to show you. So here is the Whisper library. Again, whisper is open source. You can read here. The whole explanation of how it works, it just works great in my experience. It's very good at what it does. So I really don't have a need to use anything else. So the way the code is working is I'm first checking whether transcription Txt exists. Again, this is a three and a half hour video, so I don't want to transcribe the video over and over again. So I'm going to check if the transcription exists. If it doesn't, then I'm going to download the video and transcribe the video. If it does already. If it exists already, I'm just going to skip all of this. That's what this line is doing. I'm using the YouTube class from Py YouTube. It's just a library that you can install will give you access to YouTube videos. So I'm just initializing the YouTube class here or the YouTube instance here, and then I'm going to grab the audio from the YouTube. That's the only thing I care. I don't care about the video, only the audio. So I'm going to grab the audio, I'm going to initialize the whisper model. I'm using the base model. This is not the most accurate model that Whisper has to offer, but it's very, very good at a very decent speed. If I were to use an even more accurate model, it's going to take longer to transcribe the video. So again, depending on what you're doing, just consider that there are models that are going to be even more accurate than the one that I'm using here. So I'm initializing or creating the Whisper model here and then I'm going to just download the audio to a file and I'm going to transcribe. There is a function called transcribe. Whisper has supports that function. I'm going to transcribe that file and I'm just going to grab the text and save it to a file. So if I open the file, you'll see it here? I have it because I did it before. You will see this is just obviously three and a half hour conversation. So it's a block of text with all the entire conversation between corpati and Lex. All right, so that's awesome. I have the transcription. So here I'm showing you. So I'm going to run this. It's not going to do anything because the file is in there. This cell here opens the file and just displays the first 100 characters just to make sure that everything is in there. Good. And now I'm going to be invoking my chain. So, very simple, I'm going to grab my chain, we already defined it before, and I'm going to invoke the chain, passing my context, which is going to be the whole transcription and passing the question. And the question is, is reading papers a good idea? They talked about that, so I want to know what their opinion is about that. And notice that I added a try except exception here for a good reason. When I run this, what is going to happen is that we are going to get a 400 error code back. So it's not going to work. Now, the message is where you're going to find the clue of what's happening. Here it says this model's maximum context length is 16,000 tokens, 16,385 tokens. However, your message resulted in 47,000 tokens, like, I don't know, like three times more tokens than what the model supports. Please reduce the length of the message in English. What this is telling us is the transcription is too long, and the model does not support such a long transcription. Now the error talks about tokens. Let me explain really quick without this being a video about tokenization. Let me explain what this is. Remember, these models that are processing text are using behind the scenes. It's just a big neural network, and we have to find a way to vectorize the text, turn text into numbers for the neural network to work. Like the neural network is not going to work directly with letters, with characters, right? It's going to work with numbers. So we have to find that translation between the text and the numbers, and that it's using a process, or we use a process to do the translation, or that conversion. That's called tokenization, where we tokenize text. Now, here is a link, and I added that link here in the notebook. I'm going to show you here. This is tick tokenizer. It's just a website that's published online that you can use. And specifically I'm going to be using the CL 100K base tokenizer, which is the one that's using OpenAI behind the scenes. And notice here that, hello, this is the year 2024. I'm just typing a sentence. And over here on your right side, you're going to see how many tokens come out of this sentence. It says, hello, this is the year 2024. Notice that with colors you get what the tokens are, right? So 990, six is going to be the word hello. 420 is going to be space. This right, 374 is going to be space is right. Now, notice this is funny. The year 2024. 2024 is not going to be tokenized as a single value. Instead, there are two tokens for the year 2024. You get 202, which is 23, 66, and you get four, which is 19. Okay? And there are a bunch of quirky situation. Is this correct? And is this? And this is correct. All right, so bunch of ways to represent things here. Notice this. So is lowercase or capitalize is going to be 39, 57. All capital is a different token is 1669. Space is 374. So notice these three is concepts are represented with different tokens here. Now, there are multiple reasons for that to happen. You can play with this tick tokenizer here to see how it works or why not. This is what's important. Just to remember. As a general rule, think of tokens as 75% of the number of words that you have. So for 1000 words, you can count approximately. They're going to be like 750 tokens, approximately. So that's roughly what it comes out to after you tokenize a text. But when the model, when we get an error saying that the model has a maximum context length of 16,000 tokens, you can do the math there and determine how many words you can actually fit. Remember, that's how big that prompt is going to be when we send it to the model. Obviously, in a three and a half hour video, there is just too much context. Okay? And you can go beyond YouTube videos, you can think about a company, which is a very common request right now. They want to process their knowledge base with a large language model, but they have gigabytes of data, not only three and a half hour transcript, they have gigabytes of data that they want to process with a large language model. Let's say they're creating a chat bot for their customers to ask questions about the company, about the products of the company. The company cannot fit their entire knowledge base in a single prompt, so they need to find ways to split to chunk their entire knowledge base, split that into most relevant context, like if you were doing this information or if you were doing this problem, sorry. And you wanted to solve it manually, what will be the first idea that comes to mind in my case? Well, obviously we cannot send the whole transcript to the model. We need to somehow send only a portion of the transcript that makes sense for the model to use to answer that question. Okay, so let's say we have two classes, and one class is a math class and another class is a history class. And both of those classes are within our knowledge base. If the user asks a question about math, we don't need to send the entire class of the entire history class to the model. We will only send the math class. And if the user asks anything that's related to history, then we will only send the history class. So we need to find a way within this transcription to somehow find and select the portion of the transcription that could potentially be helpful for the model to answer the question. And that is precisely what makes rack applications a little bit more complex to build that selection process. That chunking and selection process is what makes these applications a little bit more complex. So this is sort of like this idea represented here in a diagram, right? We want to send the question to our model, but somehow we want to take that transcript, split it into documents, and only send the document to the model that is relevant for that question. That is our challenge right now. How do we do that? Okay, that is what we want to do. All right, so in order to get there, the first thing that we need to do, we obviously need to split our transcript. We cannot work with the whole thing. So let's find a way to just chunk it up, just split it into separate portions. So to get there, I'm going to be using a text loader. So we're working with a text file. So I'm just going to use a simple text loader. This is another component from lanchain. And as you can see, the text loader is just simply loading that file in memory. It's going to be easier for me to work, like connecting this text loader later to a splitter so we can actually split text. So I'm going to load this text document or this file into a variable called text documents. I'm printing out here text documents. Let me run this. And you're going to see. It says, well, there is a document here and the content is going to be the entire transcript. Okay, I can keep sort of like scrolling going to see the entire transcript. That's what happens when I load this text document in memory. Now we need to find a way to split it, and there are many different mechanisms to split text. I have a link here, let me open that link really quick. So if I open this link and I'll open it up here. All right, you're going to find the types of text splitters that you can use that are supported by Langchain. And there is recursive, there is HTML. If you're using HTML documents and you want to split HTML documents, well, the HTML text splitter knows about characters, knows about HTML characters, so it's a good idea to use it. You get markdown. If you're processing code files, you get a code splitter, supports Python and Javascript, right? You get a token, a character, and you get a semantic chunker. This is a very interesting one. Okay, so this just splits the document in a smart way. Again, it's experimental, but anyway, here you can find the entire list of splitters and we are going to be using a very simple one. We are going to be using, let me just execute this. We're going to be using a recursive character text splitter. So this is the way it works. Okay, we're going to take our entire transcript and we're going to specify a predefined length, like how many characters we want on every chunk or every document. And then we're going to specify a little bit of overlap. So we're going to get, let's say from the character zero to character 100 and that's going to be the first document. And then we're going to go back 20 characters. That's going to be our overlap and go from character 80 to character 180 and then go back 20 characters and do the same thing. So we're going to have that overlap between documents. Here is the example. Let me run this. I'm going to use the recursive character text splitter and I'm specifying a chunk size of 101 hundred is just too small. But just to show you here how it works, I'm going to probably set it to 1000 later, but you're going to see chunk size 100 and the overlap of 20. And I'm going to split the document that we loaded here, which is just a big document at this point. And then I'm just displaying the last five or the first five documents here that we created. So notice this, it says, I think it's possible that physics has exploits. That's how this document starts. Notice that the document ends saying to find them arranging some and the document ends there. And if we go to the second document, it says arranging some kind of a crazy quantum. See how there is an overlap of 20 characters there. Let's see how this document ends. The second document ends with you buffer overflow. And then the third document says buffer overflow somehow gives you a rounding. So that is the way I'm splitting this document. Okay, so I have a huge transcript that I'm going to be going and chunking out into documents of specified length. Now 100 here is just for me to illustrate what this looks like. But 100 is probably not enough. So I'm just doing 1000 here, just execute the same thing, but now doing it 1000, you can try, or this is going to be a hyperparameter of your system that you're going to have to try and experiment with. If you're going to be using recursive character text splitter, you might want to try with 2000, maybe 3000. Change the overlap depending on your document. This is something that you're going to have to try it out. All right, so having done that, now what we have is what we wanted. So we have a bunch of smaller documents and each one of those documents will fit the context, which is good, right? We don't need to send the transcript anymore. The problem however is that we need to understand which of those documents to send to the model. Like we get a question about artificial intelligence, how do we know which of those smaller 1000 word documents are the ones that we need to combine and send to the model? That is the question that we need to answer next. And this is one of the most fascinating topics of working with large language models and working on these type of systems that you are going to find. And it's the idea of embeddings. Okay, so let me show you first what this looks like here. Okay. And you're going to have to trust me for 1 second and then I'm going to make this a little bit more clear. So this is my proposal. So if we start with the transcript and we split it into documents, we already did that and we have all of those smaller documents. Imagine that there is a magic formula for us to compare a question with all of those documents. And that magic formula that I'm calling computing similarity here, that magic formula will tell us which of those smaller documents are the most similar to the question that the user asked. And using that similarity we could return the documents that are the most similar. So we could select the documents that we want to send as context to the model. So the process of doing that, I'm saying, this magic formula, this actually exists. We can generate embeddings for every one of those documents and generate embeddings for the question. Now, what an embedding is, is just a vector in space. You can think of coordinates in a multi dimensional space of where a specific idea is located. Okay? So here is the way it works. Imagine that I talk about books. Grab a book. This is the deep learning with Python book from Francois Cholet. And I'm talking about books. And I generate the location in this multi dimensional space of where you would put this book, and that book will live here. In this location here. I don't know if you can see it in the camera. I probably can, but imagine, actually, let me just put this here. Let's say I locate this book, that corner over there, okay? That is the coordinates where this book is going to be located as a concept in my world. And now I grab a can, okay? So I grab a can, and this is a very different concept. So it's not going to go close to the book. It's probably going to go to a different corner of my room. So the coordinates representing this can are going to go somewhere else. And now I'm going to grab another book. This is a human in the loop book. This is a very similar concept to this book. So if I were to locate this, if I were to generate coordinates for this book, those coordinates will probably place these two books very close to each other, over that corner and very far apart from the can that's located on that. So hopefully these are like 3d idea makes sense. But the embedding is basically a function. For now, we can call it a magic function that given a concept, given an idea, a text, a word, an image, it locates, it sort of like generates a coordinate in multidimensional space of where that concept, idea, object should appear. And related concepts are going to be located very close to each other, while separate concepts, concepts that are not similar, should be located far apart. So let me give you one specific example, and I'm going to be using. For this example, I'm going to be using the cohere playground. Cohere is an amazing company. They have a very good large language model. They have a playground, which is what I'm using here. And that playground, I'm using their embedding section of the playground to generate the location of several ideas that I added here. So you're going to notice I have seven different sentences. Mary's sister is Susanna. John and Tommy are brothers. Patricia likes white cars. Pedro's mother is a teacher. Lucia drives an Audi. Mary has two siblings, and mercedes are amazing automobiles. Okay, so seven different sentences, and I generated embeddings for all of these sentences. What cohere is going to do is going to generate those locations, those vectors. But what's really cool about this playground is that those vectors, by the way, they have multiple dimensions, because those are coordinates in multidimensional space. But cohere is sort of like displaying a compressed version of those vectors in two dimensions so we can visualize and see what happens with those vectors. So this is the output after we did that. So the first thing that you are going to notice is that there are four sentences toward the left and three sentences toward the right. So let's explore which sentences cohere decided to group together. So, sentence number three, Patricia likes white cars, is close to mercedes or amazing automobiles, and is close to Lucia, drives an Audi. So the three sentences that somehow talk about cars are close to each other. Now, this is what's really cool about this. Okay? Notice that the sentence says, lucia drives an Audi. It doesn't say the word car. It doesn't say the word automobile. It says audi. The third sentence uses the words white cars. And somehow cohere's model knows that white cars is a similar concept than Audi's, or audi, in this case, same thing with automobile and mercedes. See how these three concepts are together. Now, let's see this. Here you get John and Tommy are brothers, and that's close to Mary's sister is Susanna. That's close to Mary, has two siblings. Notice how these three sentences are close together. They're talking about sisters, brothers, siblings. And then you get Pedro's mother is a teacher, which is a little bit farther apart from the sibling idea, but still close by. Embeddings are an amazing idea that makes everything that you see here possible. Okay, so if we go back to our diagram, if we generate embeddings, and this is this section that says embed here. If we generate. If we can generate embeddings for each one of the documents that we generated. Right. Each one of the chucks, we generate embeddings for them, and we also generate the embedding for the question, we can then compute the similarity, how similar. How close are those embeddings? And basically grab the most similar chunks or the most similar documents and use those most similar documents as the context for our model. So if our content was about the question that the user is asking is about cars, and we generate the embeddings of all of these questions, then we could potentially send these three chunks, these three sentences, three, five and seven, to answer a question about cars. If the question was about family relationships, then we will send the closest of these four here, or maybe all four if they fit right. That is the idea behind using embeddings to solve the problem with the transcription. So how do we do that? Well, you'll see. Fortunately, OpenAI or not OpenAI, but langchain thought about this, and they offer the ability to use embeddings from OpenAI. So they have a class, it's called the OpenAI embeddings that we can use to automatically generate. Not automatically, but generate the embeddings for anything. Right. So here's one example I'm creating this instance. It's called embeddings. And I'm embedding a query, who is Mary'sister? Okay, so I'm going to generate this embedding here by calling embedding query, and then I'm printing out the result. And you'll see this is the embedding, and it's just a long, long vector. Remember, this is just a location in a multidimensional space of where that query is going to leave. Just wanted to show you what it's going to generate. The length or the number of dimensions is 1536. So 1536 dimensions here. That's how many little numbers here or small numbers here are going to be within that vector. Now, that. That is interesting. We're going to use that later, but for now we have a way, using OpenAI or using langchain to generate the embeddings. Let's see a little bit more. Let's see how these embeddings work. Now, just to make sure we have all of the ideas that we need to solve this problem, I'm going to generate now two more embeddings, in this case, embeddings for two separate sentences. Okay, so the query was who is Mary'sister? The sentences are, Mary's sister is Susanna, and Pedro's mother is teacher. Okay, so obviously I generated three embeddings. What I want to show you is how we can compute how close these embeddings are to each other. And if everything is working as it should, then the embedding for who is Mary's sister should be closer to Mary's sister is Susanna, because that is the answer, the direct answer to the question that I'm asking. How do I do that? Well, I'm going to be using cosine similarity. There are many different ways to compute, to take two vectors or two embeddings and compute how close they are to each other. The most popular way by far is cosine similarity. So I'm using psychilearn. Psychilearn provides cosine similarity functions so I don't have to implement it. It's not hard to implement, but anyway, I don't have to. And then I'm going to be computing the cosine similarity between the query, the embedded query and sentence one, and the embedded query and sentence two. Okay, so these are the two similarities. And when I print that out, let me execute this line, by the way, when I print that out, you're going to see that the similarity between the query who is Mary'sister with the first sentence. The first sentence was Mary'sister is Susanna. The similarity was zero. 91. Remember, similarity of one will be perfect. Zero will be very, very dissimilar. And the similarity of the second sentence is zero. 76. So clearly the embeddings are working. I'm getting that Mary Sister Isusana is the correct answer, or it's the closest, not the correct answer. I'm sorry. It's the closest of the two sentences to the query that I asked. Okay, awesome. So how do we use the concept of embeddings? Right? We already know that we can use embeddings. We already have a chain, but we need to find a way to take that transcript and put it all together with our chain so the process works without any interruption. So there is one more problem that we need to solve, and is that if you think about it, if you get a three and a half hour transcript, that's a huge amount of content. And we split that. We have many, many different documents, potentially thousands of documents that we're going to have to go through and compute similarity when every time we answer a question, we're going to have to compute the embedding of the question, compute the embeddings of every single document, and then find which of those documents are the most similar. There is a lot of processing there. Fortunately, we don't have to do any of that by hand because there is a new idea. It's called a vector store, a new idea here for us in the video. Obviously, it's a vector store, which is a database for vectors, a database for embeddings. That's very simple. And the whole idea of a database for embeddings is that we can use that database, number one, to store all of our content, all of the documents from that transcription. Number two, to automatically generate the embeddings for all of those documents and store those embeddings so we don't have to regenerate them with every question. And number three, and it's the most important one, vector stores are optimized to do similarity search really quick. So that means that we can give a vector store an embedding and say, find me this many documents or the top three documents that are the closest to this embedding. And the vector store will do that really quick, which is precisely what we need for this. Right? So let's set up really quick a vector store. This is how the process is going to look, right? We're going to get our transcription, split it into documents. We already did that. We're going to get all of those documents and we're going to generate the embeddings for all of those documents. That's going to happen behind the scenes. All of those embeddings are going to get stored in a vector store. And then that vector store is the one that we're going to use to get the question and produce the most similar chunks or the most similar documents from that list. Okay, so for starters, and just for this example here, we're going to take it to pine cone in a second. I'm going to be using a docker raid in memory search vector store. This is just a vector store that's going to work in memory, in the memory of your computer. So I don't have anything else to set up. And just for the example, I'm going to use the same seven sentences that I showed you in the cohere playground. So these are the seven sentences that I'm loading into my vector memory vector store. So notice here, I'm going to run this how I'm saying, hey, just generate this memory search from these texts. So I'm just loading that into memory and I'm using, this is important, I'm telling this vector store which class it should use to generate the embeddings. And because we're using the OpenAI embeddings here, we're using the OpenAI Mpi, we need to use the embedding model from OpenAI. If you were using a llama model, you will need to use a different way to generate embeddings. If you were using the cohere model. You will need to generate embeddings with the cohere embedding model. Okay, in this case, I'm using the embedding models from the OpenAI API. Cool. So after executing this, I have vector store one. That's the name of the variable which is the vector store. And now notice this, I can just call similarity search with score, just a function. And I can pass a query who is Mary'sister? And I can specify how many chunks I want back. By default it's going to return four. I'm just specifying three here just for the example. But just so you know, by default you're going to get the top four most similar documents back. So I'm saying who is Mary'sister? And I'm getting in order. Mary'sister is Susanna. Mary has two siblings. John and Tommy are brothers. Those are the three most similar documents from this vector store to this question here. Awesome. All right, so how do we connect this vector store to the previous chain? So we're going to need a new concept that's called a retriever. Okay, very simple. You'll see how it works right now. So this is, remember that our previous chain required two parameters, required a context and it required a question. And remember, our previous chain started with the prompt and we passed that context and we passed that question. Now it's just going to look a little bit different. So now the way it's going to look is, well, we're going to add a retriever in front of the prompt. That retriever is the class that will take care of connecting to the vector store and retrieving the documents that will become the context. Okay. That's why you see here that the context is coming now from the retriever into the prompt. Okay, so let me show you the code and how to create, how to set up a retriever really quick from any vector store here you can just generate a retriever. By the way, there are multiple retrievers are not only connected to vector store, you can have a retriever that's going to run a different algorithm behind the scenes to select different documents. Like for example, like a page rank algorithm to do search like Google does, for example, and select documents using that algorithm. In this case, I'm generating my retriever directly from the vector store. So I'm saying, hey, off of this vector store one, just give me a retriever. And now if I invoke my retriever with a question, the retriever behind the scenes will take care of everything. The retriever is generating an embedding for that question is sending that to the vector store and is retrieving the top four documents and sending them back. That's all of that is done by the retriever. The retriever is sort of like the gateway, so I can connect it to a chain, and the retriever is the gateway to that vector store. Notice here who is Mary's sister. It's returning. I'm going to just run it. It's returning. Mary's sister is Susanna. Mary has two siblings, John and Tommy are brothers, et cetera. Okay, so the retriever is working. Now we need to connect that retriever to the prompt. Okay? The prompt expects two parameters, the context and the question. Okay, so this is how you can connect the retriever to the prompt. Remember that before when we generated the translation prompt, I show you a little bit of syntax sugar to do curly braces to specify the parameters here. I'm just going to unravel that a bit. We're going to be using a class that's called runnable parallel, which is a lang chain class that is going to let us do two things at the same time. Okay? So the two things are very simple in this case, I'm calling this a setup. And the setup is, well, the context is going to come from the retriever one. It's the one that we just created. That's where the context is coming from the retriever. And the question is going to be a pass through. So I'm taking the question, I need to specify the question because the retriever, it's using the question, so it needs the question. But I'm also telling this runnable parallel class to pass that question through the next step in the process, because the prompt requires that question as well. Retriever requires the question because it needs to generate the similar documents. But the prompt, the following component on the chain, when we connect this retriever to the chain, will need that question. Therefore, I'm going to do a runnable pass through, okay? And now you can see here how this runnable pass through is just going to return a map. Exactly what my prompt is expecting. So I invoke it with what's a great car? Tell me what's a great car? And you're going to see that the context that it returns is Mercedes are amazing automobiles. Notice I have to just 1 second appreciate how good embeddings are. But I'm using the words great and car, and the idea is, well, Mercedes instead of car and automobiles instead of car, and amazing instead of great. And still these two concepts are very close to each other. So just close parentheses there on my amusement. So anyway, when I invoke this runnable parallel class with that question, I'm going to get a map back with a context variable containing an array with all the four documents that are the most similar. And a question. And the reason the question is coming back is because it's a runnable pass through, and because it's a runnable pass through, it's just going to pass it through the output as well. And the question is, what's a great car? All right, so now I can get this set up and put it together as part of my chain. Okay, it's very simple. Now I have my chain here and I can add the setup, connect it to the prompt, connect it to the model connected to the parser, and I can ask my question, what color is Patricia's car? And if I run this, it's going to say it's white and it's getting that. So it's going to the setup, it's going to the vector store. Remember, the setup is going to call the retriever. It connects to the vector store, find the top documents that are the most important ones to answer this particular question. Then it's taking that, connecting all of that into the prompt template, the prompt template going to the model, the model outputting into the parser. The parser is just giving me that final string. The second question is, what's a great car? And then the answer is going to be Mercedes. So it's getting that answer from the prompt. All right, so all of this is good, but we really need to do this with the transcription, which is the huge document. Right now. It's just sample. So let's load the transcription into the vector store. This is going to be very simple at this point. I have my vector story memory class here, and there is a from documents. And I can pass just the list of chunks that we created. Remember that we split the whole transcription into multiple documents. Well, I can just pass that here. By the way, let me just check, because I don't know what is the length of document? Like, how many chunks do we have? We have 221 chunks. And just to make sure that we're working here with what I want, I just want to see just document zero. I think it's possible that physics has exploits, blah, blah, blah. That's the first document here. All right, so awesome. So 221 documents I'm loading into a new vector store that I'm calling vector store two. Okay. And remember, I need to pass, what are the embeddings that I'm going to be using? How am I going to be generating those embeddings? What is the class? And these embeddings is the OpenAI embeddings that we generated. Cool. Awesome. So now that I have here, this is the syntax sugar back again. Instead of using the runnable parallel, I could use the runnable Parallel, but instead of using that, I'm just going to be using this quickly thingy here, the squiggly braces here. Notice that I'm saying, okay, so the context is going to come from a retriever that I'm generating from vector store two. And the question is a runnable pass through, because I need to pass that to the prompt as well. Then connect it to the prompt, then connect it to the model, then connect it to the parser. Okay. And by the way, this is exactly the same, and I'm just going to show you here, just so it's clear, so this here that we just did is exactly the same as if we do setup two runable parallel context. Yes, this is cool. Copilot generated all of that code really quick. So I'm going to put setup two. All right, so these two are equivalent. I'm just showing you a little bit of syntax sugar on how to do that. Let me run this here. Well, maybe not, because begtor store two doesn't exist, because I need to run this first. Okay, now let me run it. Okay, so this is my chain, and actually I need to invoke the chain. If I want something to happen, I'm going to need to invoke the chain. What car does Lucia drive? Let me ask a question. It says, I don't know. That's awesome. Of course not, because this is using the vector store two, which is the vector stores that's connected to the transcript. So let me just do something. What is Agi? Let's see. I don't know. I don't even know if they answered this question. AGI stands for artificial general intelligence. Okay, that's good. And the second chain, again, this is exactly the same thing as I did before, but instead of using the runnable parallel class directly, I'm just using the syntax sugar with the squiggly braces. What is synthetic intelligence? When I run that, and then it's going to answer, synthetic intelligence is described as the next stage of development in the context, blah, blah, blah. This answer is coming from the transcript. Now we have it in this vector store. Cool. We are almost done. Just one more step and it's that that vector store is in memory at this point. We don't want that vector store to be in memory. We want it to be physically stored somewhere in an actual vector database. There are many, many different vector stores that you can use here. I'm going to be using pine cone. It's very popular. So I created a pine cone account before recording this video. You can see it here. And I have a project that I call YouTube and I'm going to create an index here. It's really fast. I'm going to create an index and I'm going to call it YouTube index. Okay. So I'm going to call it YouTube index. What does it say? Can only contain lowercase letters. Didn't I, YouTube. Okay, YouTube index. That's going to be the name of my index. I'm going to copy that because I'm going to knit it and then it says how many dimensions am I going to use? Remember it's 1536 dimensions. You can also go here, set up by model and you can find, this is the embedding model that we're using. So it's 1536 dimensions. This is the number of dimensions the embeddings have. Okay. So because we're using the OpenAI, we have 1536 dimensions. Okay, awesome. So I'm going to click create index. This should take just a second to have this index ready. There we go. No records yet. Awesome. I'm going to go now back to my code. And here in my code you're going to find a variable that's called index name. And I need to paste here the name of the index that I use, which is YouTube index. And I'm going to run this cell and what this cell is going to do. Notice we already did this before for the memory store, the memory vector store where we loaded all of the documents that we generated in that vector store. In this case, I need to specify the documents that I'm going to be loading. Choose the transcript chunks from the transcript, the embedding model that I'm going to be using and an index name. So I'm going to do this. This is loading. Just going to take a few seconds. And that's it. 3.5 seconds is done. Let's go back to pine cone. And if I refresh this screen, we should see, hopefully. There we go. So we should see all of the vectors already here in this index. So notice that you get the source, which is the TXT file, the text. This is the entire text of this chunk or the original document. This is the content that we give the model later. And we also get the vector, which I don't see here. You know what it might be because, well, maybe not. It doesn't show like the actual vector values, not that is important. But anyway, so I have everything here in pine cone. So now the rest of the code is just the same that we saw before. Right? I can show you that this is working by just running a similarity search over Pine cone. We already did this before with the memory database. And what is Hollywood going to start doing? That is just a question. I'm basically saying return documents that are similar to this question. I'm getting three documents back because I'm just limiting here how many documents are going to come back and then I'm going to set up a new chain. But this time I'm using pine cone. So I'm passing the retriever coming from pine cone and everything else is exactly the same. And when we execute that, you're going to get, Hollywood is going to start using AI to generate scenes, et cetera, et cetera. So it's answering that question from the Pine cone database. That's live. So that's it. So hopefully this makes sense. Hopefully this showed you a little bit more about embeddings, a little bit more about tokenization, and a little bit more about how to use land chain to put all of this together. I'm going to be recording more videos related to lanchain and large language models in the coming few weeks. So yeah, stay tuned if you want to see more of these videos. And let me know in the comments if you have any questions or if you have any topics that you would like me to discuss. Thank you and bye.